{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02e9e8a-0c3f-45d8-97e2-8167959dbcc7",
   "metadata": {},
   "source": [
    "# Training Temporal Convolutional Neural Network on Landsat Satellite Image Time Series to Detect Leafy Spurge\n",
    "\n",
    "This Notebook aims to develop a temporal convolutional neural network based on a time series of Landsat satellite imagery and class labels from the National Land Cover Dataset (NLCD). \n",
    "\n",
    "Table of Contents\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "\n",
    "Contributors:\n",
    "Thomas Lake\n",
    "\n",
    "Version: November 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d8e7-dd9b-47c5-a2e9-69e94b7c220f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a4b5b7-8512-48aa-a4bd-a4d5d3e08db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f71c6c-4490-4309-8c1d-d7f2963cf999",
   "metadata": {},
   "source": [
    "# Tensorflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8049345-4da0-4684-8993-8e29555897db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 11:28:18.443354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/moeller/lakex055/.conda/envs/tf_gpu/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow setup.\n",
    "\n",
    "# Tensorflow version 2.4.1\n",
    "import tensorflow as tf\n",
    "print(tf.__version__) \n",
    "\n",
    "# Keras setup.\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Flatten\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten, Lambda, SpatialDropout1D, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, History, EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a367f-0a4f-4e04-9d34-33956132e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are We Using a GPU?\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "print(tf.test.is_built_with_cuda)\n",
    "# <function is_built_with_cuda at 0x7f4f5730fbf8>\n",
    "\n",
    "print(tf.test.gpu_device_name())\n",
    "# /device:GPU:0\n",
    "\n",
    "print(tf.config.get_visible_devices())\n",
    "# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "# Matrix multiplication test with gpu\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d955a9e-a0ea-434b-91c7-8ed3c29918f6",
   "metadata": {},
   "source": [
    "# Import temporalCNN Python modules\n",
    "# https://github.com/charlotte-pel/temporalCNN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b4809-2424-403f-b176-8abdf9ad8501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Import from ~/sits folder\n",
    "# Contains readingsits.py file to read and compute spectral features on SITS\n",
    "sys.path.append(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN/sits\")\n",
    "import readingsits\n",
    "\n",
    "# Import from ~/deeplearning folder\n",
    "# Contains multiple .py files with varying DL architectures \n",
    "sys.path.append(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN/deeplearning\")\n",
    "\n",
    "import architecture_features\n",
    "import architecture_complexity\n",
    "import architecture_rnn\n",
    "import architecture_regul\n",
    "import architecture_batchsize\n",
    "import architecture_depth\n",
    "import architecture_spectro_temporal\n",
    "import architecture_pooling\n",
    "\n",
    "# Import from ~/outputfiles folder\n",
    "# Contains evaluation.py and save.py files with fucntions to compute summary statistics, write predictions, and create confusion matrices\n",
    "sys.path.append(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN/outputfiles\")\n",
    "\n",
    "import evaluation\n",
    "import save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c95330-a69e-4a72-9465-3a2946f6432c",
   "metadata": {},
   "source": [
    "Define Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e473b6b-7e31-45db-80c6-8506e1757535",
   "metadata": {},
   "source": [
    "Read in Full Training/Testing/Validation Dataset sampled from 1M points & NLCD classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d299484f-0d0b-4952-8dfb-7d99f2685f15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727297, 67)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'X0_BlueMarchApril2000',\n",
       " 'X0_GreenMarchApril2000',\n",
       " 'X0_RedMarchApril2000',\n",
       " 'X0_NIRMarchApril2000',\n",
       " 'X0_SWIR1MarchApril2000',\n",
       " 'X0_SWIR2MarchApril2000',\n",
       " 'X0_NDVIMarchApril2000',\n",
       " 'X0_BlueMayJune2000',\n",
       " 'X0_GreenMayJune2000',\n",
       " 'X0_RedMayJune2000',\n",
       " 'X0_NIRMayJune2000',\n",
       " 'X0_SWIR1MayJune2000',\n",
       " 'X0_SWIR2MayJune2000',\n",
       " 'X0_NDVIMayJune2000',\n",
       " 'X0_BlueJulyAug2000',\n",
       " 'X0_GreenJulyAug2000',\n",
       " 'X0_RedJulyAug2000',\n",
       " 'X0_NIRJulyAug2000',\n",
       " 'X0_SWIR1JulyAug2000',\n",
       " 'X0_SWIR2JulyAug2000',\n",
       " 'X0_NDVIJulyAug2000',\n",
       " 'X1_BlueMarchApril2001',\n",
       " 'X1_GreenMarchApril2001',\n",
       " 'X1_RedMarchApril2001',\n",
       " 'X1_NIRMarchApril2001',\n",
       " 'X1_SWIR1MarchApril2001',\n",
       " 'X1_SWIR2MarchApril2001',\n",
       " 'X1_NDVIMarchApril2001',\n",
       " 'X1_BlueMayJune2001',\n",
       " 'X1_GreenMayJune2001',\n",
       " 'X1_RedMayJune2001',\n",
       " 'X1_NIRMayJune2001',\n",
       " 'X1_SWIR1MayJune2001',\n",
       " 'X1_SWIR2MayJune2001',\n",
       " 'X1_NDVIMayJune2001',\n",
       " 'X1_BlueJulyAug2001',\n",
       " 'X1_GreenJulyAug2001',\n",
       " 'X1_RedJulyAug2001',\n",
       " 'X1_NIRJulyAug2001',\n",
       " 'X1_SWIR1JulyAug2001',\n",
       " 'X1_SWIR2JulyAug2001',\n",
       " 'X1_NDVIJulyAug2001',\n",
       " 'X2_BlueMarchApril2002',\n",
       " 'X2_GreenMarchApril2002',\n",
       " 'X2_RedMarchApril2002',\n",
       " 'X2_NIRMarchApril2002',\n",
       " 'X2_SWIR1MarchApril2002',\n",
       " 'X2_SWIR2MarchApril2002',\n",
       " 'X2_NDVIMarchApril2002',\n",
       " 'X2_BlueMayJune2002',\n",
       " 'X2_GreenMayJune2002',\n",
       " 'X2_RedMayJune2002',\n",
       " 'X2_NIRMayJune2002',\n",
       " 'X2_SWIR1MayJune2002',\n",
       " 'X2_SWIR2MayJune2002',\n",
       " 'X2_NDVIMayJune2002',\n",
       " 'X2_BlueJulyAug2002',\n",
       " 'X2_GreenJulyAug2002',\n",
       " 'X2_RedJulyAug2002',\n",
       " 'X2_NIRJulyAug2002',\n",
       " 'X2_SWIR1JulyAug2002',\n",
       " 'X2_SWIR2JulyAug2002',\n",
       " 'X2_NDVIJulyAug2002',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'class']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data into pandas df from .csv\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Dataframe contains N rows, 67 columns\n",
    "df = pd.read_csv(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2001_full_dataset_oct2022.csv\", header='infer')\n",
    "print(df.shape)\n",
    "\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03360b29-7772-48b1-88b9-52851e87566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of class instances in the 2001 full dataset\n",
    "df['class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09402e9-a4d4-496d-8d9b-ec13d7185f34",
   "metadata": {},
   "source": [
    "Format the data frame for use with the temporalCNN modules. Sample dataset format found in temporalCNN/example folder\n",
    "\n",
    "1. Drop the first column ('Unnamed: 0') and the latitude/longitude columns.\n",
    "2. Move the class column to the first index\n",
    "3. Add an index column to the second position (a unique ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7c92a-bd43-4a9e-9830-19343c1ee698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first column (\"Unnamed:0\"), and latitude/longitude columns\n",
    "df.drop(['Unnamed: 0', 'lat', 'lon'], axis=1, inplace=True)\n",
    "\n",
    "# Move class column (\"class\") to first index\n",
    "cls_column = df.pop(\"class\")\n",
    "df.insert(0, \"class\", cls_column)\n",
    "\n",
    "# Set the class column as the index\n",
    "df.set_index('class', inplace=True)\n",
    "\n",
    "# Add column in second position as an unique ID (\"index\")\n",
    "df.insert(0, \"index\", range(1, 1 + len(df)))\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea0241-ecab-4c33-8a6c-89888ab2a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05bc1c54-0957-4984-845d-de4412be0dca",
   "metadata": {},
   "source": [
    "# Divide full dataset into training and testing subsets, export to csv\n",
    "\n",
    "Use Scikit Learn train test split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Quick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7fcd9-7469-42df-b680-636c678e0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "#Number of rows/columns in dataset\n",
    "#print(train_df.shape, test_df.shape)\n",
    "\n",
    "# Write full training and testing dataframes to CSV\n",
    "# Export format as rows [classID, index, band_values/timeseries...] with no header\n",
    "# train_df.to_csv(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/train_dataset_2001_full_oct2022.csv\", header=False)\n",
    "# test_df.to_csv(\"/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/test_dataset_2001_full_oct2022.csv\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b60a1-0c93-4df5-861f-66b26ed2c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Block - Keep this!\n",
    "# Combine multiple CSV files with identical columns into one CSV & export\n",
    "\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "#path = r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22' # use your path\n",
    "\n",
    "#all_files = glob.glob(os.path.join(path, \"train_*.csv\"))\n",
    "\n",
    "#df = pd.concat((pd.read_csv(f, header=None) for f in all_files), ignore_index=True)\n",
    "\n",
    "#df.to_csv('/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/train_dataset_allyears_full_oct22.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c2bb4-d264-4bc4-bf9e-5c25024c760c",
   "metadata": {},
   "source": [
    "# Set Model Paths for Input Data & Model Results Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f176d-3683-407b-8a8a-8b87847dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to exported training/testing dataset\n",
    "sits_path = '/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22'    \n",
    "    \n",
    "# Set Architecture / Model Run Index (used if running in batch on MSI)\n",
    "noarchi = 0\n",
    "norun = 0\n",
    "feature = \"SB\" #use only spectral bands provided (do not compute new bands, like NDVI, which are already computed)\n",
    "\n",
    "# Parameters to set\n",
    "n_channels = 7 #-- B G NDVI NIR Red SWIR1 SWIR2\n",
    "val_rate = 0.1 # Validation data rate\n",
    "\n",
    "# String variables for the training and testing datasets\n",
    "train_str = 'train_dataset_allyears_full_oct22'\n",
    "test_str = 'test_dataset_allyears_full_oct22'\t\t\t\t\t\n",
    "\n",
    "# Get filenames\n",
    "train_file = sits_path + '/' + train_str + '.csv'\n",
    "test_file = sits_path + '/' + test_str + '.csv'\n",
    "print(\"train_file: \", train_file)\n",
    "print(\"test_file: \", test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cd820-8c87-4dd1-8201-5705f3130299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set a model results path\n",
    "res_path = '/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN'\n",
    "\n",
    "# Creating output path if does not exist\n",
    "if not os.path.exists(res_path):\n",
    "  print(\"ResPath DNE\")\n",
    "  os.makedirs(res_path)\n",
    "\n",
    "# Evaluated metrics\n",
    "eval_label = ['OA', 'train_loss', 'train_time', 'test_time']\t\n",
    "\t\n",
    "\n",
    "# Output files\t\t\t\n",
    "res_path = res_path + '/Archi' + str(noarchi) + '/'\n",
    "if not os.path.exists(res_path):\n",
    "  os.makedirs(res_path)\n",
    "  print(\"noarchi: \", noarchi)\n",
    "\n",
    "# Create output files to capture model results\n",
    "str_result = feature + '-' + train_str + '-noarchi' + str(noarchi) + '-norun' + str(norun) \n",
    "res_file = res_path + '/resultOA-' + str_result + '.csv'\n",
    "res_mat = np.zeros((len(eval_label),1))\n",
    "traintest_loss_file = res_path + '/trainingHistory-' + str_result + '.csv'\n",
    "conf_file = res_path + '/confMatrix-' + str_result + '.csv'\n",
    "out_model_file = res_path + '/bestmodel-' + str_result + '.h5'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75179cea-943f-4612-a2d4-1b8985112908",
   "metadata": {},
   "source": [
    "# Read, Reshape, and Normalize Training Dataset for Temporal CNN\n",
    "# https://github.com/charlotte-pel/temporalCNN/blob/master/run_archi.py\n",
    "\n",
    "The readSITSData function is located in temporalCNN/sits/readingsits.py \n",
    "The function outputs X (variable vectors for each sample/row), polygon_ids (unique id for each row), and Y (land class label for each sample/row)\n",
    "\n",
    "I am not sure if normalizing the data here will cause issues when predicting on (non-normalized) Landsat imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5dce8-f294-4908-a857-277e69e1b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Read in SITS training and testing datasets\n",
    "X_train, polygon_ids_train, y_train = readingsits.readSITSData(train_file)\n",
    "X_test,  polygon_ids_test, y_test = readingsits.readSITSData(test_file)\n",
    "print(X_test)  #verify spectral band data looks correct\n",
    "print(X_test.shape) #num_samples, 63 bands (9 timesteps * 7 bands/timestep = 63)\n",
    "\n",
    "\n",
    "# Number of unique classes in y_train and y_test datasets should = 9\n",
    "n_classes_test = len(np.unique(y_test))\n",
    "print(n_classes_test)\n",
    "n_classes_train = len(np.unique(y_train))\n",
    "print(n_classes_train)\n",
    "\n",
    "# heck equal number of classes in training and testing dataset\n",
    "if(n_classes_test != n_classes_train):\n",
    "  print(\"WARNING: different number of classes in train and test\")\n",
    "\n",
    "n_classes = max(n_classes_train, n_classes_test) # 9 classes\n",
    "y_train_one_hot = to_categorical(y_train) # specify number of classes explicity - may need to recode classes sequentially (1-9) to work correctly?\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "print(y_test_one_hot) #verify one hot encoding was successful\n",
    "print(y_test_one_hot.shape)\n",
    "print(y_test_one_hot[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4b0cd-1ae8-4bff-876d-04e685b09cfc",
   "metadata": {},
   "source": [
    "# Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afea172-a1ff-4a32-a02a-fc67e6f8e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = readingsits.addingfeat_reshape_data(X_train, feature, n_channels) #Feature = \"SB\" (spectral bands)\n",
    "\n",
    "X_test = readingsits.addingfeat_reshape_data(X_test, feature, n_channels)\t\t\n",
    "\n",
    "print(X_train[0, :, :])\n",
    "print(X_train.shape) #verify reshape was successful, now num_samples, num_timesteps, num_bands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3a4f8-1879-45cf-a143-b1bfdae360ff",
   "metadata": {},
   "source": [
    "# Create the Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3395ee7-ed4d-4e56-ace0-bf7e07685f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Extracting a validation set (if necesary)\n",
    "if val_rate > 0:\n",
    "  #Number of samples to take from Training dataset based on validation rate\n",
    "  val_num_samples = int(math.ceil(X_train.shape[0] * val_rate))\n",
    "\n",
    "  #Select random indices for val_num_samples to select validation set\n",
    "  val_indices = random.sample(range(1, X_train.shape[0]), val_num_samples)\n",
    "  #remove these indices from the training set\n",
    "  train_indices = np.delete(range(1, X_train.shape[0]), val_indices)\n",
    "\n",
    "  #Create training and validation sets \n",
    "  X_val = X_train[val_indices, :]\n",
    "  y_val = y_train[val_indices]\n",
    "  X_train = X_train[train_indices, :]\n",
    "  y_train = y_train[train_indices]\n",
    "\n",
    "  #--- Computing the one-hot encoding (recomputing it for train)\n",
    "  y_train_one_hot = to_categorical(y_train)\n",
    "  y_val_one_hot = to_categorical(y_val)\n",
    "\n",
    "  n_classes_val = len(np.unique(y_val))\n",
    "  print(n_classes_val)\n",
    "  n_classes_train = len(np.unique(y_train))\n",
    "  print(n_classes_train)\n",
    "\n",
    "  #Check equal number of classes in training and testing dataset\n",
    "  if(n_classes_val != n_classes_train):\n",
    "    print(\"WARNING: different number of classes in train and test\")\n",
    "  \n",
    "\n",
    "print(X_train.shape, y_train_one_hot.shape, X_val.shape, y_val_one_hot.shape, X_test.shape, y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d410edc-6b8d-4377-a7cd-81ddb0126c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------\t\t\n",
    "def conv_bn(X, **conv_params):\t\n",
    "\tnbunits = conv_params[\"nbunits\"];\n",
    "\tkernel_size = conv_params[\"kernel_size\"];\n",
    "\n",
    "\tstrides = conv_params.setdefault(\"strides\", 1)\n",
    "\tpadding = conv_params.setdefault(\"padding\", \"same\")\n",
    "\tkernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
    "\tkernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "\n",
    "\tZ = Conv1D(nbunits, kernel_size=kernel_size, \n",
    "\t\t\tstrides = strides, padding=padding,\n",
    "\t\t\tkernel_initializer=kernel_initializer,\n",
    "\t\t\tkernel_regularizer=kernel_regularizer)(X)\n",
    "\n",
    "\treturn BatchNormalization(axis=-1)(Z) #-- CHANNEL_AXIS (-1)\n",
    "\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def conv_bn_relu(X, **conv_params):\n",
    "\tZnorm = conv_bn(X, **conv_params)\n",
    "\treturn Activation('relu')(Znorm)\n",
    "\t\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def conv_bn_relu_drop(X, **conv_params):\t\n",
    "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
    "\tA = conv_bn_relu(X, **conv_params)\n",
    "\treturn Dropout(dropout_rate)(A)\n",
    "\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def fc_bn(X, **fc_params):\n",
    "\tnbunits = fc_params[\"nbunits\"];\n",
    "\t\n",
    "\tkernel_regularizer = fc_params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
    "\tkernel_initializer = fc_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "\t\t\n",
    "\tZ = Dense(nbunits, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(X)\n",
    "\treturn BatchNormalization(axis=-1)(Z) #-- CHANNEL_AXIS (-1)\n",
    "\t\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def fc_bn_relu(X, **fc_params):\t\n",
    "\tZnorm = fc_bn(X, **fc_params)\n",
    "\treturn Activation('relu')(Znorm)\n",
    "\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def fc_bn_relu_drop(X, **fc_params):\n",
    "\tdropout_rate = fc_params.setdefault(\"dropout_rate\", 0.5)\n",
    "\tA = fc_bn_relu(X, **fc_params)\n",
    "\treturn Dropout(dropout_rate)(A)\n",
    "\n",
    "#-----------------------------------------------------------------------\t\t\n",
    "def softmax(X, nbclasses, **params):\n",
    "\tkernel_regularizer = params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
    "\tkernel_initializer = params.setdefault(\"kernel_initializer\", \"glorot_uniform\")\n",
    "\treturn Dense(nbclasses, activation='softmax', \n",
    "\t\t\tkernel_initializer=kernel_initializer,\n",
    "\t\t\tkernel_regularizer=kernel_regularizer)(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced00f9d-845e-4604-acb6-dcb8c0492f8b",
   "metadata": {},
   "source": [
    "# Create the Deep Learning Model in Keras\n",
    "\n",
    "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss. Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See the Keras loss function docs, the TensorFlow categorical identity docs and the tf.one_hot docs for details).\n",
    "\n",
    "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer. Once the dataset has been prepared, define the model, compile it, fit it to the training data. See the Keras Sequential model guide for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96e1e7-b390-444a-8714-40b7c7fcab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Custom Model Architecture inspired by Allred at al. 2021\n",
    "\n",
    "#Get input sizes\n",
    "m, L, depth = X_train.shape\n",
    "input_shape = (L, depth)\n",
    "\n",
    "#-- parameters of the architecture\n",
    "nbclasses = 10\n",
    "l2_rate = 1.e-6\n",
    "dropout_rate = 0.1\n",
    "nb_conv = 3\n",
    "nb_fc= 1\n",
    "nbunits_conv = 64 #-- will be double\n",
    "nbunits_fc = 256 #-- will be double\n",
    "\n",
    "\t# Define the input placeholder.\n",
    "X_input = Input(input_shape)\n",
    "\t\t\n",
    "\t#-- nb_conv CONV layers\n",
    "X = X_input\n",
    "for add in range(nb_conv):\n",
    "    X = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
    "\t#-- Flatten + \t1 FC layers\n",
    "X = Flatten()(X)\n",
    "for add in range(nb_fc):\t\n",
    "    X = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
    "\t\t\n",
    "\t#-- SOFTMAX layer\n",
    "out = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
    "\t\t\n",
    "\t# Create model.\n",
    "model = Model(inputs = X_input, outputs = out, name='Archi_3CONV64_1FC256')\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7955d-e104-489a-98cf-ad0d95931050",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Define Model Variables\n",
    "###\n",
    "\n",
    "# Model variables\n",
    "n_epochs = 1000\n",
    "batch_size = 2048\n",
    "#lr = 0.0001 #recommended in Allred et al., 2021\n",
    "#beta_1 = 0.9 #not used, but can be used to modify optimizer LR\n",
    "#beta_2 = 0.999\n",
    "#epsilon = 1e-07\n",
    "\n",
    "#Define Class Weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights =  class_weight.compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train),\n",
    "                                        y = y_train)\n",
    "\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Class weights function\n",
    "# squared of inverse frequenecy weights?\n",
    "# inverse of frequency\n",
    "class_weights = {0: 0,\n",
    "                 1: 7.046028630719989,\n",
    "                 2: 3.6421837069230087,\n",
    "                 3: 31.37461158722999,\n",
    "                 4: 0.7614511317372198,\n",
    "                 5: 0.6015453322153169,\n",
    "                 6: 0.3652990948014909,\n",
    "                 7: 0.39487324200412083,\n",
    "                 8: 4.334510403657227,\n",
    "                 9: 20.275284755853498}\n",
    "\n",
    "print(class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7480cfb-9dc4-419a-950f-3eedfc875c23",
   "metadata": {},
   "source": [
    "# Model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ea703-c441-4c13-a7d7-2ca4011c56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(out_model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "#Plot Loss and Accuracy Callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.f1 = []\n",
    "        self.val_f1 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.f1.append(logs.get('accuracy'))\n",
    "        self.val_f1.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.f1, label=\"Acc\")\n",
    "        ax2.plot(self.x, self.val_f1, label=\"val Acc \")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLearning()\n",
    "\n",
    "\n",
    "# Learning Rate Warmup and Decay Callback\n",
    "def lr_warmup_cosine_decay(global_step,\n",
    "                           warmup_steps,\n",
    "                           hold = 0,\n",
    "                           total_steps=0,\n",
    "                           start_lr=0.0,\n",
    "                           target_lr=1e-3):\n",
    "    # Cosine decay\n",
    "    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n",
    "\n",
    "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
    "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
    "\n",
    "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
    "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
    "    if hold > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold,\n",
    "                                 learning_rate, target_lr)\n",
    "    \n",
    "    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "#Plot the learning rate schedule\n",
    "steps = np.arange(0, 1000, 1)\n",
    "lrs = []\n",
    "\n",
    "for step in steps:\n",
    "  lrs.append(lr_warmup_cosine_decay(step, total_steps=len(steps), warmup_steps=50, hold=5))\n",
    "plt.plot(lrs)\n",
    "\n",
    "\n",
    "class WarmupCosineDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n",
    "\n",
    "        super(WarmupCosineDecay, self).__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.hold = hold\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = 0\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = model.optimizer.lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n",
    "                                    total_steps=self.total_steps,\n",
    "                                    warmup_steps=self.warmup_steps,\n",
    "                                    start_lr=self.start_lr,\n",
    "                                    target_lr=self.target_lr,\n",
    "                                    hold=self.hold)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        \n",
    "# If already batched\n",
    "# If not batched\n",
    "total_steps = n_epochs\n",
    "# 5% of the steps\n",
    "warmup_steps = int(0.05*total_steps)\n",
    "\n",
    "warmup_callback = WarmupCosineDecay(total_steps=total_steps, \n",
    "                             warmup_steps=warmup_steps,\n",
    "                             hold=int(warmup_steps/2), \n",
    "                             start_lr=0.0, \n",
    "                             target_lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbff7a5-48fa-45fc-a6ad-4dde8edc1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model Optimizer\n",
    "#opt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer = 'adam', loss = \"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Train the model\n",
    "start_train_time = time.time()\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(x = X_train, \n",
    "                 y = y_train_one_hot, \n",
    "                 epochs = n_epochs,\n",
    "                 batch_size = batch_size, \n",
    "                 shuffle=True,\n",
    "                 validation_data=(X_val, y_val_one_hot), \n",
    "                 verbose=1,\n",
    "                 callbacks = [plot_losses, warmup_callback])\n",
    "\n",
    "\n",
    "train_time = round(time.time()-start_train_time, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339d5a6-eef4-496d-8955-cdbfe3a2bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model prediction\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Predict the model on withheld testing dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_pred_flat = y_pred.flatten()\n",
    "y_pred_flat = y_pred_flat.astype(int)\n",
    "\n",
    "y_test = y_test.astype(int)    \n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "class_names = [\"Water\", \"Developed\", \"BarrenLand\", \"Forest\", \"Shrub/Scrub\", \"Grassland/Herbaceous\", \"Croplands\", \"EmergentWetlands\", \"LeafySpurge\"]\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "c = multilabel_confusion_matrix(y_test_flat, y_pred_flat, labels = class_labels)\n",
    "model_output_metrics = []\n",
    "for i in range(len(class_labels)):\n",
    "    tn=c[i, 0, 0]\n",
    "    tp=c[i, 1, 1]\n",
    "    fn=c[i, 1, 0]\n",
    "    fp=c[i, 0, 1]\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    TPR_Sens_Recall = tp/(tp+fn)\n",
    "    TNR_Spec = tn/(tn+fp)\n",
    "    FPR = fp/(fp+tn)\n",
    "    FNR = fn/(fn+tp)\n",
    "    precision = tp/(tp+fp)\n",
    "    jaccard = tp/(tp+fp+fn)\n",
    "    beta = 0.5\n",
    "    F05 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    beta = 1\n",
    "    F1 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    beta = 2\n",
    "    F2 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    outputs = [class_names[i], tp, tn, fp, fn, accuracy, TPR_Sens_Recall, TNR_Spec, FPR, FNR, precision, jaccard, F1]\n",
    "    model_output_metrics.append(outputs)\n",
    "\n",
    "# Print and format outputs\n",
    "print(tabulate(model_output_metrics, floatfmt=\".2f\", headers=[\"Class Name\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"TPR/Sens/Recall\", \"TNR/Spec\", \"FPR\", \"FNR\", \"Precision\", \"Jaccard\", \"F1\"]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165add2-2c67-442b-a8ff-bb89420478b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix, cohen_kappa_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score, fbeta_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tabulate import tabulate\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        classes,\n",
    "        test_name,\n",
    "        normalize=False,\n",
    "        set_title=False,\n",
    "        save_fig=False,\n",
    "        cmap=plt.cm.Blues\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if set_title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    # and save it to log file\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "        #with open(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_log.txt', 'ab') as f:\n",
    "        #    f.write(b'\\nNormalized confusion matrix\\n')\n",
    "        #    np.savetxt(f, cm, fmt='%.3f')\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        #with open(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_log.txt', 'ab') as f:\n",
    "        #    f.write(b'\\nConfusion matrix, without normalization\\n')\n",
    "        #    np.savetxt(f, cm, fmt='%7u')\n",
    "\n",
    "    print(cm)\n",
    "    #cm = cm[1:10]\n",
    "    #cm = cm[:,1:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    if normalize:\n",
    "        im.set_clim(0., 1.)     # fixes missing '1.0' tick at top of colorbar\n",
    "    cb = ax.figure.colorbar(im, ax=ax)\n",
    "    if normalize:\n",
    "        cb.set_ticks(np.arange(0., 1.2, 0.2))\n",
    "        cb.set_ticklabels([f'{i/5:.1f}' for i in range(6)])\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title if set_title else None,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    ax.set_ylim(len(cm)-0.5, -0.5)\n",
    "    ax.xaxis.label.set_size(10)\n",
    "    ax.yaxis.label.set_size(10)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if np.round(cm[i, j], 2) > 0.:\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                ax.text(j, i, 'â€“',\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        if normalize:\n",
    "            plt.savefig(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_cm_normal.pdf')\n",
    "        else:\n",
    "            plt.savefig(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_cm_non_normal.pdf')\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inv_category_dict = {1:\"Water\", 2:\"Developed\", 3:\"BarrenLand\", 4:\"Forest\", 5:\"Shrub/Scrub\", 6:\"Grassland/Herbaceous\", 7:\"Croplands\", 8:\"EmergentWetlands\", 9:\"LeafySpurge\"}\n",
    "\n",
    "class_names = [inv_category_dict[i] for i in np.arange(1, 10)]\n",
    "\n",
    "\n",
    "# save plot of normalized cm\n",
    "plot_confusion_matrix(\n",
    "    y_test_flat,\n",
    "    y_pred_flat,\n",
    "    classes=class_names,\n",
    "    test_name=\"myModel\",\n",
    "    normalize=True,\n",
    "    save_fig=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0a2c-b399-4b4e-916e-a67860a00e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "\n",
    "\n",
    "# Predict the model on withheld testing dataset\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "\n",
    "def computingRMSE(y_test_one_hot,p_test):\n",
    "\t\"\"\"\"\n",
    "\t\tComputing RMSE from the prediction of the softmax layer\n",
    "\t\tINPUT:\n",
    "\t\t\t- y_test_one_one: one hot encoding of the test labels\n",
    "\t\t\t- p_test: predicted 'probabilities' from the model for the test instances\n",
    "\t\tOUTPUT:\n",
    "\t\t\t- rmse: Root Mean Square Error\n",
    "\t\"\"\"\n",
    "\tnbTestInstances = y_test_one_hot.shape[0]\n",
    "\tdiff_proba = y_test_one_hot - p_test\n",
    "\treturn math.sqrt(np.sum(diff_proba*diff_proba)/nbTestInstances)\n",
    "\n",
    "print(computingRMSE(y_test_one_hot, y_pred_probs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cb6e6-2001-4568-96c2-42193ff8a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the Trained Model as a .h5 file\n",
    "model.save(r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN/Archi0/secondmodel.h5')\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b334d1-f867-47a1-9dfb-dece5ed2d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a trained model\n",
    "model = keras.models.load_model(r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/temporalCNN/Archi0/draft_cnn_modeL_300epochs_oct272022.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb47699-ba2e-48d7-ae1c-0ed2c5df9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model Prediction on small TIF raster file\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "\n",
    "# Not used, but might be needed to index across tiles\n",
    "tile_index = 170\n",
    "\n",
    "# Input prediction .tif path\n",
    "image_path = r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/rasters_gcloud/'\n",
    "\n",
    "# Output prediction file path\n",
    "outpath = r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/raster_predictions_gcloud/'\n",
    "\n",
    "# List all .tif files in /rasters folder for prediction\n",
    "tif_image_list = glob.glob(image_path + '*.tif')\n",
    "\n",
    "print(tif_image_list[0:2])\n",
    "\n",
    "# Loop through every tif file for prediction.\n",
    "for t in range(len(tif_image_list)):\n",
    "    \n",
    "    # Open .tif array image with rasterio, read to numpy array\n",
    "    with rasterio.open(tif_image_list[t], 'r') as ds:\n",
    "        arr = ds.read()  # read all raster values\n",
    "\n",
    "    # Define shape of input .tif image\n",
    "    bands, width, height = arr.shape\n",
    "    \n",
    "    # Convert Data Type to float32 by division.\n",
    "    arr = arr/10000\n",
    "    \n",
    "    # Reshape .tif array axes for correct format so model can predict.\n",
    "    arr = np.moveaxis(arr, 0, -1) #move axis to channels last\n",
    "    new_arr = arr.reshape(-1, arr.shape[-1]) #reshape to row and column\n",
    "    num_pixels = width*height\n",
    "    new_arr2 = new_arr.reshape(num_pixels, 9, 7)\n",
    "    print(new_arr2.shape)\n",
    "\n",
    "    # Predict model and reshape to export.\n",
    "    p = model.predict(new_arr2) # p is prediction from the DL model\n",
    "    pim = p.reshape(width, height, 10) # Dimension of prediction in rows, columns, bands (10 classes)\n",
    "    pim2 = np.moveaxis(pim, 2, 0) # move axis so bands is first\n",
    "    \n",
    "    # ArgMax for Segmentation.\n",
    "    pim3 = np.argmax(pim2, axis=0) # take softmax of predictions for segmentation\n",
    "    print(pim3.shape)\n",
    "\n",
    "    # Get the file name (landsat_image_170_t.tif) by splitting input path.\n",
    "    fileout_string = os.path.split(tif_image_list[t])\n",
    "        \n",
    "    # Output prediction raster .\n",
    "    out_meta = ds.meta.copy()\n",
    "\n",
    "    # Get Output metadata.\n",
    "    out_meta.update({'driver':'GTiff',\n",
    "                     'width':ds.shape[1],\n",
    "                     'height':ds.shape[0],\n",
    "                     'count':1,\n",
    "                     'dtype':'float64',\n",
    "                     'crs':ds.crs, \n",
    "                     'transform':ds.transform,\n",
    "                     'nodata':0})\n",
    "\n",
    "    # Write predicted raster to file.\n",
    "    with rasterio.open(fp=outpath + \"/prediction_\" + fileout_string[-1], #outputpath_name\n",
    "                 mode='w',**out_meta) as dst:\n",
    "                 dst.write(pim3, 1) # the numer one is the number of bands\n",
    "            \n",
    "    print(\"Writing file...\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016a3f8-3a54-455c-8bc2-45b55e1cd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "\n",
    "# Not used, but might be needed to index across tiles\n",
    "tile_index = 170\n",
    "\n",
    "# Input prediction .tif path\n",
    "image_path = r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/rasters_gcloud/'\n",
    "\n",
    "# Output prediction file path\n",
    "outpath = r'/panfs/roc/groups/7/moeller/shared/leafy-spurge-demography/datasets_oct22/raster_predictions_gcloud/'\n",
    "\n",
    "# List all .tif files in /rasters folder for prediction\n",
    "tif_image_list = glob.glob(image_path + '*.tif')\n",
    "\n",
    "print(tif_image_list[0:2])\n",
    "\n",
    "# Loop through every tif file for prediction.\n",
    "for t in range(len(tif_image_list)):\n",
    "    \n",
    "    # Open .tif array image with rasterio, read to numpy array\n",
    "    with rasterio.open(tif_image_list[t], 'r') as ds:\n",
    "        arr = ds.read()  # read all raster values\n",
    "\n",
    "    # Define shape of input .tif image\n",
    "    bands, width, height = arr.shape\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f911526-6286-442f-97ad-d7b238f3a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(arr.shape)\n",
    "\n",
    "print(arr.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0693886-780d-4308-a8fc-7cda470fadfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b215f-69cb-4c8a-9b63-d226c2e26547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc562e-429a-4bf0-9649-3d566dc53387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthengine",
   "language": "python",
   "name": "earthengine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
